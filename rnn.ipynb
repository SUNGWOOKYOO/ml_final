{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set parameters\n",
    "#ROOT = 'PATH/TO/data/processed/'\n",
    "ROOT = '/home/kddlab/swyoo/data/'\n",
    "PATH_TO_TRAIN = ROOT + 'train_data.csv'\n",
    "PATH_TO_TEST = ROOT + 'test_data.csv'\n",
    "checkpoint_dir = './checkpoint'\n",
    "if not os.path.exists(checkpoint_dir): os.mkdir(checkpoint_dir)\n",
    "        \n",
    "layers = 1\n",
    "rnn_size = 100\n",
    "batch_size = 50\n",
    "drop_keep_prob = 0.7\n",
    "\n",
    "n_epochs = 10\n",
    "learning_rate = 0.001\n",
    "decay = 0.96\n",
    "decay_steps = 1e4\n",
    "grad_cap = 0\n",
    "print_step = 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "data = pd.read_csv(PATH_TO_TRAIN)\n",
    "valid = pd.read_csv(PATH_TO_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['SessionId','ItemId','timestamp'] \n",
    "valid.columns = ['SessionId','ItemId','timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(['SessionId', 'timestamp'], ascending=[True,True])\n",
    "valid = valid.sort_values(['SessionId', 'timestamp'], ascending=[True,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2454710"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 824 ms, sys: 23.1 ms, total: 847 ms\n",
      "Wall time: 835 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ItemIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1214492</th>\n",
       "      <td>0000c0fb51e11</td>\n",
       "      <td>673981</td>\n",
       "      <td>1541240881</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214493</th>\n",
       "      <td>0000c0fb51e11</td>\n",
       "      <td>673981</td>\n",
       "      <td>1541240881</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214494</th>\n",
       "      <td>0000c0fb51e11</td>\n",
       "      <td>673981</td>\n",
       "      <td>1541240887</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214495</th>\n",
       "      <td>0000c0fb51e11</td>\n",
       "      <td>10369176</td>\n",
       "      <td>1541240932</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214496</th>\n",
       "      <td>0000c0fb51e11</td>\n",
       "      <td>10369176</td>\n",
       "      <td>1541240932</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             SessionId    ItemId   timestamp  ItemIdx\n",
       "1214492  0000c0fb51e11    673981  1541240881        0\n",
       "1214493  0000c0fb51e11    673981  1541240881        0\n",
       "1214494  0000c0fb51e11    673981  1541240887        0\n",
       "1214495  0000c0fb51e11  10369176  1541240932        1\n",
       "1214496  0000c0fb51e11  10369176  1541240932        1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add item index \n",
    "itemids = data['ItemId'].unique()\n",
    "n_items = len(itemids)\n",
    "itemidmap = pd.Series(data=np.arange(n_items), index=itemids).to_dict()\n",
    "%time data['ItemIdx'] = data['ItemId'].map(lambda x: itemidmap[x])\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  5, 15, 20, 32], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## offset sessions\n",
    "offset_sessions = np.zeros(data['SessionId'].nunique()+1, dtype=np.int32)\n",
    "offset_sessions[1:] = data.groupby('SessionId').size().cumsum()\n",
    "offset_sessions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kddlab/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-9-b16dc61c218e>:19: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-b16dc61c218e>:21: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/kddlab/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "## placeholder & learning rate\n",
    "X = tf.placeholder(tf.int32, [batch_size], name='input')\n",
    "Y = tf.placeholder(tf.int32, [batch_size], name='output')\n",
    "States = [tf.placeholder(tf.float32, [batch_size, rnn_size], name='rnn_state') for _ in range(layers)]\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "lr = tf.maximum(1e-5,tf.train.exponential_decay(\n",
    "    learning_rate, global_step, decay_steps, decay, staircase=True\n",
    ")) \n",
    "\n",
    "## gru weigths\n",
    "with tf.variable_scope('gru_layer', reuse=tf.AUTO_REUSE):\n",
    "    initializer = tf.glorot_uniform_initializer()\n",
    "    embedding = tf.get_variable('embedding', [n_items, rnn_size], initializer=initializer)\n",
    "    softmax_W = tf.get_variable('softmax_w', [n_items, rnn_size], initializer=initializer)\n",
    "    softmax_b = tf.get_variable('softmax_b', [n_items], initializer=tf.zeros_initializer())\n",
    "    \n",
    "## gru_cell\n",
    "with tf.variable_scope('gru_cell', reuse=tf.AUTO_REUSE):\n",
    "    cell = tf.nn.rnn_cell.GRUCell(rnn_size, activation=tf.nn.tanh)\n",
    "    drop_cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=drop_keep_prob)\n",
    "    stacked_cell = tf.nn.rnn_cell.MultiRNNCell([drop_cell] * layers)\n",
    "\n",
    "## feedforward gur_cell\n",
    "inputs = tf.nn.embedding_lookup(embedding, X)\n",
    "output, state_ = stacked_cell(inputs, tuple(States))\n",
    "final_state = state_\n",
    "\n",
    "### for training\n",
    "sampled_W = tf.nn.embedding_lookup(softmax_W, Y)\n",
    "sampled_b = tf.nn.embedding_lookup(softmax_b, Y)\n",
    "logits = tf.matmul(output, sampled_W, transpose_b=True) + sampled_b\n",
    "### cross-entropy loss\n",
    "# yhat = tf.nn.softmax(logits)\n",
    "# cost = tf.reduce_mean(-tf.log(tf.diag_part(yhat)+1e-24))\n",
    "### bpr loss\n",
    "yhat = logits\n",
    "yhatT = tf.transpose(yhat)\n",
    "cost = tf.reduce_mean(-tf.log(tf.nn.sigmoid(tf.diag_part(yhat)-yhatT)))\n",
    "### top1 loss\n",
    "# yhat = logits\n",
    "# yhatT = tf.transpose(yhat)\n",
    "# term1 = tf.reduce_mean(tf.nn.sigmoid(-tf.diag_part(yhat)+yhatT)+tf.nn.sigmoid(yhatT**2), axis=0)\n",
    "# term2 = tf.nn.sigmoid(tf.diag_part(yhat)**2) / batch_size\n",
    "# cost = tf.reduce_mean(term1 - term2)\n",
    "\n",
    "### for prediction\n",
    "logits_all = tf.matmul(output, softmax_W, transpose_b=True) + softmax_b\n",
    "yhat_all = tf.nn.softmax(logits_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking Loss \n",
    "\n",
    "BPR loss <br>\n",
    "$L_s = - \\frac{1}{N_s} \\cdot \\sum_{j=1}^{N_s} \\text{log}(\\sigma(\\hat{r}_{s,i} -\\hat{r}_{s,j}))$ <br>\n",
    "$N_s$: sample size，$\\hat{r}_{s,i}$: yhat of positive sample， $\\hat{r}_{s,j}$: yhat of positive sample\n",
    "\n",
    "TOP loss <br>\n",
    "$L_s = \\frac{1}{N_s} \\cdot \\sum_{j=1}^{N_s} (\\sigma(\\hat{r}_{s,j} - \\hat{r}_{s,i})) +\\sigma(\\hat{r^2_j})$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kddlab/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "## optimize\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "tvars = tf.trainable_variables()\n",
    "gvs = optimizer.compute_gradients(cost, tvars)\n",
    "if grad_cap > 0:\n",
    "    capped_gvs = [(tf.clip_by_norm(grad, grad_cap), var) for grad, var in gvs]\n",
    "else:\n",
    "    capped_gvs = gvs \n",
    "train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## session start\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\tStep 1\tlr: 0.00100\tloss: 0.6932\tElapsed: 0.4\n",
      "Epoch 0\tStep 1000\tlr: 0.00100\tloss: 0.4175\tElapsed: 16.8\n",
      "Epoch 0\tStep 2000\tlr: 0.00100\tloss: 0.3467\tElapsed: 33.3\n",
      "Epoch 0\tStep 3000\tlr: 0.00100\tloss: 0.3056\tElapsed: 49.7\n",
      "Epoch 0\tStep 4000\tlr: 0.00100\tloss: 0.2767\tElapsed: 66.2\n",
      "Epoch 0\tStep 5000\tlr: 0.00100\tloss: 0.2513\tElapsed: 82.6\n",
      "Epoch 0\tStep 6000\tlr: 0.00100\tloss: 0.2314\tElapsed: 99.0\n",
      "Epoch 0\tStep 7000\tlr: 0.00100\tloss: 0.2144\tElapsed: 115.5\n",
      "Epoch 0\tStep 8000\tlr: 0.00100\tloss: 0.2008\tElapsed: 131.9\n",
      "Epoch 0\tStep 9000\tlr: 0.00100\tloss: 0.1888\tElapsed: 148.4\n",
      "Epoch 0\tStep 10000\tlr: 0.00100\tloss: 0.1782\tElapsed: 164.8\n",
      "Epoch 0\tStep 11000\tlr: 0.00096\tloss: 0.1691\tElapsed: 181.2\n",
      "Epoch 0\tStep 12000\tlr: 0.00096\tloss: 0.1609\tElapsed: 197.6\n",
      "Epoch 0\tStep 13000\tlr: 0.00096\tloss: 0.1535\tElapsed: 214.0\n",
      "Epoch 0\tStep 14000\tlr: 0.00096\tloss: 0.1468\tElapsed: 230.5\n",
      "Epoch 0\tStep 15000\tlr: 0.00096\tloss: 0.1409\tElapsed: 246.9\n",
      "Epoch 0\tStep 16000\tlr: 0.00096\tloss: 0.1353\tElapsed: 263.4\n",
      "Epoch 0\tStep 17000\tlr: 0.00096\tloss: 0.1304\tElapsed: 279.8\n",
      "Epoch 0\tStep 18000\tlr: 0.00096\tloss: 0.1257\tElapsed: 296.2\n",
      "Epoch 0\tStep 19000\tlr: 0.00096\tloss: 0.1214\tElapsed: 312.6\n",
      "Epoch 0\tStep 20000\tlr: 0.00096\tloss: 0.1175\tElapsed: 329.1\n",
      "Epoch 0\tStep 21000\tlr: 0.00092\tloss: 0.1138\tElapsed: 345.5\n",
      "Epoch 0\tStep 22000\tlr: 0.00092\tloss: 0.1104\tElapsed: 361.9\n",
      "Epoch 0\tStep 23000\tlr: 0.00092\tloss: 0.1072\tElapsed: 378.4\n",
      "Epoch 0\tStep 24000\tlr: 0.00092\tloss: 0.1042\tElapsed: 394.8\n",
      "Epoch 0\tStep 25000\tlr: 0.00092\tloss: 0.1015\tElapsed: 411.2\n",
      "Epoch 0\tStep 26000\tlr: 0.00092\tloss: 0.0988\tElapsed: 427.6\n",
      "Epoch 0\tStep 27000\tlr: 0.00092\tloss: 0.0963\tElapsed: 444.1\n",
      "Epoch 0\tStep 28000\tlr: 0.00092\tloss: 0.0940\tElapsed: 460.5\n",
      "Epoch 0\tStep 29000\tlr: 0.00092\tloss: 0.0919\tElapsed: 477.0\n",
      "Epoch 0\tStep 30000\tlr: 0.00092\tloss: 0.0899\tElapsed: 493.4\n",
      "Epoch 0\tStep 31000\tlr: 0.00088\tloss: 0.0879\tElapsed: 509.8\n",
      "Epoch 0\tStep 32000\tlr: 0.00088\tloss: 0.0861\tElapsed: 526.3\n",
      "Epoch 0\tStep 33000\tlr: 0.00088\tloss: 0.0843\tElapsed: 542.7\n",
      "Epoch 0\tStep 34000\tlr: 0.00088\tloss: 0.0827\tElapsed: 559.1\n",
      "Epoch 0\tStep 35000\tlr: 0.00088\tloss: 0.0811\tElapsed: 575.6\n",
      "Epoch 0\tStep 36000\tlr: 0.00088\tloss: 0.0795\tElapsed: 592.0\n",
      "Epoch 0\tStep 37000\tlr: 0.00088\tloss: 0.0781\tElapsed: 608.5\n",
      "Epoch 0\tStep 38000\tlr: 0.00088\tloss: 0.0767\tElapsed: 624.9\n",
      "Epoch 0\tStep 39000\tlr: 0.00088\tloss: 0.0754\tElapsed: 641.3\n",
      "Epoch 0\tStep 40000\tlr: 0.00088\tloss: 0.0741\tElapsed: 657.7\n",
      "Epoch 0\tStep 41000\tlr: 0.00085\tloss: 0.0729\tElapsed: 674.1\n",
      "Epoch 0\tStep 42000\tlr: 0.00085\tloss: 0.0718\tElapsed: 690.5\n",
      "Epoch 0\tStep 43000\tlr: 0.00085\tloss: 0.0707\tElapsed: 707.0\n",
      "Epoch 0\tStep 44000\tlr: 0.00085\tloss: 0.0696\tElapsed: 723.4\n",
      "Epoch 0\tStep 45000\tlr: 0.00085\tloss: 0.0686\tElapsed: 739.8\n",
      "Epoch 0\tStep 46000\tlr: 0.00085\tloss: 0.0676\tElapsed: 756.2\n",
      "Epoch 0\tStep 47000\tlr: 0.00085\tloss: 0.0667\tElapsed: 772.7\n",
      "Epoch 1\tStep 48000\tlr: 0.00085\tloss: 0.0215\tElapsed: 789.2\n",
      "Epoch 1\tStep 49000\tlr: 0.00085\tloss: 0.0211\tElapsed: 805.6\n",
      "Epoch 1\tStep 50000\tlr: 0.00085\tloss: 0.0211\tElapsed: 822.0\n",
      "Epoch 1\tStep 51000\tlr: 0.00082\tloss: 0.0210\tElapsed: 838.4\n",
      "Epoch 1\tStep 52000\tlr: 0.00082\tloss: 0.0207\tElapsed: 854.8\n",
      "Epoch 1\tStep 53000\tlr: 0.00082\tloss: 0.0206\tElapsed: 871.2\n",
      "Epoch 1\tStep 54000\tlr: 0.00082\tloss: 0.0205\tElapsed: 887.7\n",
      "Epoch 1\tStep 55000\tlr: 0.00082\tloss: nan\tElapsed: 904.1\n",
      "Epoch 1\tStep 56000\tlr: 0.00082\tloss: nan\tElapsed: 920.5\n",
      "Epoch 1\tStep 57000\tlr: 0.00082\tloss: nan\tElapsed: 936.8\n",
      "Epoch 1\tStep 58000\tlr: 0.00082\tloss: nan\tElapsed: 953.2\n",
      "Epoch 1\tStep 59000\tlr: 0.00082\tloss: nan\tElapsed: 969.6\n",
      "Epoch 1\tStep 60000\tlr: 0.00082\tloss: nan\tElapsed: 985.9\n",
      "Epoch 1\tStep 61000\tlr: 0.00078\tloss: nan\tElapsed: 1002.3\n",
      "Epoch 1\tStep 62000\tlr: 0.00078\tloss: nan\tElapsed: 1018.7\n",
      "Epoch 1\tStep 63000\tlr: 0.00078\tloss: nan\tElapsed: 1035.1\n",
      "Epoch 1\tStep 64000\tlr: 0.00078\tloss: nan\tElapsed: 1051.4\n",
      "Epoch 1\tStep 65000\tlr: 0.00078\tloss: nan\tElapsed: 1067.8\n",
      "Epoch 1\tStep 66000\tlr: 0.00078\tloss: nan\tElapsed: 1084.2\n",
      "Epoch 1\tStep 67000\tlr: 0.00078\tloss: nan\tElapsed: 1100.6\n",
      "Epoch 1\tStep 68000\tlr: 0.00078\tloss: nan\tElapsed: 1117.0\n",
      "Epoch 1\tStep 69000\tlr: 0.00078\tloss: nan\tElapsed: 1133.4\n",
      "Epoch 1\tStep 70000\tlr: 0.00078\tloss: nan\tElapsed: 1149.8\n",
      "Epoch 1\tStep 71000\tlr: 0.00075\tloss: nan\tElapsed: 1166.2\n",
      "Epoch 1\tStep 72000\tlr: 0.00075\tloss: nan\tElapsed: 1182.6\n",
      "Epoch 1\tStep 73000\tlr: 0.00075\tloss: nan\tElapsed: 1199.0\n",
      "Epoch 1\tStep 74000\tlr: 0.00075\tloss: nan\tElapsed: 1215.4\n",
      "Epoch 1\tStep 75000\tlr: 0.00075\tloss: nan\tElapsed: 1231.7\n",
      "Epoch 1\tStep 76000\tlr: 0.00075\tloss: nan\tElapsed: 1248.1\n",
      "Epoch 1\tStep 77000\tlr: 0.00075\tloss: nan\tElapsed: 1264.5\n",
      "Epoch 1\tStep 78000\tlr: 0.00075\tloss: nan\tElapsed: 1280.8\n",
      "Epoch 1\tStep 79000\tlr: 0.00075\tloss: nan\tElapsed: 1297.2\n",
      "Epoch 1\tStep 80000\tlr: 0.00075\tloss: nan\tElapsed: 1313.7\n",
      "Epoch 1\tStep 81000\tlr: 0.00072\tloss: nan\tElapsed: 1330.0\n",
      "Epoch 1\tStep 82000\tlr: 0.00072\tloss: nan\tElapsed: 1346.4\n",
      "Epoch 1\tStep 83000\tlr: 0.00072\tloss: nan\tElapsed: 1362.8\n",
      "Epoch 1\tStep 84000\tlr: 0.00072\tloss: nan\tElapsed: 1379.2\n",
      "Epoch 1\tStep 85000\tlr: 0.00072\tloss: nan\tElapsed: 1395.6\n",
      "Epoch 1\tStep 86000\tlr: 0.00072\tloss: nan\tElapsed: 1412.0\n",
      "Epoch 1\tStep 87000\tlr: 0.00072\tloss: nan\tElapsed: 1428.4\n",
      "Epoch 1\tStep 88000\tlr: 0.00072\tloss: nan\tElapsed: 1444.8\n",
      "Epoch 1\tStep 89000\tlr: 0.00072\tloss: nan\tElapsed: 1461.2\n",
      "Epoch 1\tStep 90000\tlr: 0.00072\tloss: nan\tElapsed: 1477.6\n",
      "Epoch 1\tStep 91000\tlr: 0.00069\tloss: nan\tElapsed: 1494.0\n",
      "Epoch 1\tStep 92000\tlr: 0.00069\tloss: nan\tElapsed: 1510.4\n",
      "Epoch 1\tStep 93000\tlr: 0.00069\tloss: nan\tElapsed: 1526.7\n",
      "Epoch 1\tStep 94000\tlr: 0.00069\tloss: nan\tElapsed: 1543.1\n",
      "Epoch 1: Nan error!\n",
      "1 epoch elapsed time: 1546.1742396354675\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "tic = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_cost = []\n",
    "    state = [np.zeros([batch_size, rnn_size], dtype=np.float32) for _ in range(layers)]\n",
    "    iters = np.arange(batch_size)\n",
    "    maxiter = iters.max()\n",
    "    \n",
    "    start = offset_sessions[iters]\n",
    "    end = offset_sessions[iters+1]\n",
    "    \n",
    "    finished = False\n",
    "    while not finished:\n",
    "        minlen = (end-start).min()\n",
    "        out_idx = data.ItemIdx.values[start]\n",
    "        for i in range(minlen-1):\n",
    "            in_idx = out_idx\n",
    "            out_idx = data.ItemIdx.values[start+i+1]\n",
    "            # prepare inputs, targeted outputs and hidden states\n",
    "            fetches = [cost, final_state, global_step, lr, train_op]\n",
    "            feed_dict = {X: in_idx, Y: out_idx}\n",
    "            for j in range(layers): \n",
    "                feed_dict[States[j]] = state[j]\n",
    "            \n",
    "            cost_, state, step, lr_, _ = sess.run(fetches, feed_dict)\n",
    "            epoch_cost.append(cost_)\n",
    "                \n",
    "            if step == 1 or step % print_step == 0:\n",
    "                avgc = np.mean(epoch_cost)\n",
    "                print('Epoch {}\\tStep {}\\tlr: {:.5f}\\tloss: {:.4f}\\tElapsed: {:.1f}'.\n",
    "                      format(epoch, step, lr_, avgc, time.time()-tic))\n",
    "\n",
    "        start = start+minlen-1\n",
    "        mask = np.arange(len(iters))[(end-start)<=1]\n",
    "        for idx in mask:\n",
    "            maxiter += 1\n",
    "            if maxiter >= len(offset_sessions)-1:\n",
    "                finished = True\n",
    "                break\n",
    "            iters[idx] = maxiter\n",
    "            start[idx] = offset_sessions[maxiter]\n",
    "            end[idx] = offset_sessions[maxiter+1]\n",
    "        if len(mask):\n",
    "            for i in range(layers):\n",
    "                state[i][mask] = 0\n",
    "        \n",
    "    avgc = np.mean(epoch_cost)\n",
    "    if np.isnan(avgc):\n",
    "        print('Epoch {}: Nan error!'.format(epoch, avgc))\n",
    "        break\n",
    "    saver.save(sess, '{}/gru-model'.format(checkpoint_dir), global_step=epoch)\n",
    "print(\"1 epoch elapsed time:\", time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prediction & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters\n",
    "cut_off = 20     # @20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kddlab/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/gru-model-0\n"
     ]
    }
   ],
   "source": [
    "## session restore\n",
    "### 마지막(최신) 학습 checkpoint 정보를 restore한다.\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "saver.restore(sess, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isfloat(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        \n",
    "        return False\n",
    "    \n",
    "def f1(x):\n",
    "    if not isfloat(x):\n",
    "        return  \n",
    "    elif int(float(x)) not in itemidmap:\n",
    "        return\n",
    "    else:\n",
    "        return int(itemidmap[int(float(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ItemIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>700928</th>\n",
       "      <td>000138ab4f789</td>\n",
       "      <td>1211616</td>\n",
       "      <td>1541444221</td>\n",
       "      <td>4058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700929</th>\n",
       "      <td>000138ab4f789</td>\n",
       "      <td>2423456</td>\n",
       "      <td>1541444452</td>\n",
       "      <td>8334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            SessionId   ItemId   timestamp  ItemIdx\n",
       "700928  000138ab4f789  1211616  1541444221     4058\n",
       "700929  000138ab4f789  2423456  1541444452     8334"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1067819\n"
     ]
    }
   ],
   "source": [
    "valid['ItemIdx'] = valid['ItemId'].map(f1)\n",
    "valid = valid.dropna()\n",
    "valid.ItemIdx = valid.ItemIdx.astype('int') \n",
    "display(valid[:2])\n",
    "print(len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ItemIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>700928</th>\n",
       "      <td>000138ab4f789</td>\n",
       "      <td>1211616</td>\n",
       "      <td>1541444221</td>\n",
       "      <td>4058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700929</th>\n",
       "      <td>000138ab4f789</td>\n",
       "      <td>2423456</td>\n",
       "      <td>1541444452</td>\n",
       "      <td>8334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700930</th>\n",
       "      <td>000138ab4f789</td>\n",
       "      <td>2423456</td>\n",
       "      <td>1541444609</td>\n",
       "      <td>8334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700931</th>\n",
       "      <td>000138ab4f789</td>\n",
       "      <td>152444</td>\n",
       "      <td>1541446323</td>\n",
       "      <td>4099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700932</th>\n",
       "      <td>000138ab4f789</td>\n",
       "      <td>2423456</td>\n",
       "      <td>1541446487</td>\n",
       "      <td>8334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            SessionId   ItemId   timestamp  ItemIdx\n",
       "700928  000138ab4f789  1211616  1541444221     4058\n",
       "700929  000138ab4f789  2423456  1541444452     8334\n",
       "700930  000138ab4f789  2423456  1541444609     8334\n",
       "700931  000138ab4f789   152444  1541446323     4099\n",
       "700932  000138ab4f789  2423456  1541446487     8334"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## valdation data set\n",
    "valid['ItemIdx'] = valid['ItemId'].map(lambda x: itemidmap[x])\n",
    "valid[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  11,  17, 197, 360], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## valid offset sessions\n",
    "### 위 학습과 동일하게 각 세션의 시작점의 index list를 만든다.\n",
    "offset_sessions = np.zeros(valid['SessionId'].nunique()+1, dtype=np.int32)\n",
    "offset_sessions[1:] = valid.groupby('SessionId').size().cumsum()\n",
    "offset_sessions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## init prediction\n",
    "### 예측 세션의 배치 사이즈 보다 작을 경우 배치 사이즈를 조정한다.\n",
    "if len(offset_sessions) - 1 < batch_size:\n",
    "    batch_size = len(offset_sessions) - 1\n",
    "### training step과 동일\n",
    "iters = np.arange(batch_size).astype(np.int32)\n",
    "maxiter = iters.max()\n",
    "start = offset_sessions[iters]\n",
    "end = offset_sessions[iters+1]\n",
    "in_idx = np.zeros(batch_size, dtype=np.int32)\n",
    "predict_state = [np.zeros([batch_size, rnn_size], dtype=np.float32) for _ in range(layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break at endpoint 1025935\n",
      "recall:  0.9218654203238996 mrr: 0.7211920217467956 elapsed time: 131.9587483406067\n"
     ]
    }
   ],
   "source": [
    "## prediction & evaluation\n",
    "evalutation_point_count = 0\n",
    "mrr, recall = 0.0, 0.0\n",
    "tic = time.time()\n",
    "while True:\n",
    "    ### iters는 batch placeholder로 0보다 큰 즉, 마지막 세션까지는 모든 위치를 켜두고\n",
    "    ### 아래에서 session 데이터가 다 소진되면 해당 위치를 -1로 할당할 것이다.\n",
    "    ### valid_mask가 0이 되면 즉 모든 위치가 꺼지면 학습을 종료한다.\n",
    "    valid_mask = iters >= 0\n",
    "    if valid_mask.sum() == 0:\n",
    "        print(\"break at endpoint\", evalutation_point_count)\n",
    "        break\n",
    "        \n",
    "    start_valid = start[valid_mask]\n",
    "    minlen = (end[valid_mask]-start_valid).min()\n",
    "    in_idx[valid_mask] = valid.ItemIdx.values[start_valid]\n",
    "    \n",
    "    for i in range(minlen-1):\n",
    "        out_idx = valid.ItemIdx.values[start_valid+i+1]\n",
    "        ## --- prediction --- ##\n",
    "        fetches = [yhat_all, final_state]\n",
    "        feed_dict = {X: in_idx}\n",
    "        for j in range(layers): \n",
    "            feed_dict[States[j]] = predict_state[j]\n",
    "        preds, predict_state = sess.run(fetches, feed_dict)\n",
    "        preds = pd.DataFrame(data=np.asarray(preds).T)\n",
    "        preds.fillna(0, inplace=True) ### preds shape: (item_size, batch_size)\n",
    "        ## --- evaluation --- ##\n",
    "        in_idx[valid_mask] = out_idx\n",
    "        ranks = (preds.values.T[valid_mask].T > \n",
    "                 np.diag(preds.loc[in_idx].values)[valid_mask]).sum(axis=0) + 1\n",
    "        rank_ok = ranks < cut_off\n",
    "        recall += rank_ok.sum()\n",
    "        mrr += (1.0 / ranks[rank_ok]).sum()\n",
    "        evalutation_point_count += len(ranks)\n",
    "        \n",
    "    start = start+minlen-1\n",
    "    mask = np.arange(len(iters))[(valid_mask) & (end-start<=1)]\n",
    "    \n",
    "    for idx in mask:\n",
    "        maxiter += 1\n",
    "        ## 더 이상 할당할 세션이 없으면 해당 위치에 -1을 할당하여 끈다.\n",
    "        if maxiter >= len(offset_sessions)-1:\n",
    "            iters[idx] = -1\n",
    "        else:\n",
    "            iters[idx] = maxiter\n",
    "            start[idx] = offset_sessions[maxiter]\n",
    "            end[idx] = offset_sessions[maxiter+1]\n",
    "            \n",
    "    if len(mask):\n",
    "        for i in range(layers):\n",
    "            predict_state[i][mask] = 0\n",
    "\n",
    "recall = recall/evalutation_point_count\n",
    "mrr = mrr/evalutation_point_count\n",
    "print(\"recall: \", recall, \"mrr:\", mrr, \"elapsed time:\", time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025935\n",
      "1025935\n"
     ]
    }
   ],
   "source": [
    "### evalute that all dataset is used \n",
    "print(evalutation_point_count)\n",
    "print(sum(valid.groupby('SessionId').size() - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: \n",
      "clicked_item_id:  2367\n",
      "submission:  [5743 2367 7421 6036 8587 5086 6253 5870 8900    8 7812 2362 2248    9\n",
      " 6148 9270 3082 8428 6747 2788 3617 8928 2063 5700 9105]\n",
      "score:  [0.06477986, 0.05402035, 0.0250135, 0.0190592, 0.0145876305, 0.012599608, 0.012306675, 0.012305204, 0.011211358, 0.010889083, 0.007889688, 0.00744067, 0.0071209683, 0.006755861, 0.00675532, 0.0067508984, 0.006673782, 0.006638033, 0.006569237, 0.006359222, 0.006289683, 0.0061950884, 0.0060110167, 0.005997303, 0.005979884]\n"
     ]
    }
   ],
   "source": [
    "fetches = [yhat_all, final_state]\n",
    "feed_dict = {X: in_idx}\n",
    "for j in range(layers): \n",
    "            feed_dict[States[j]] = predict_state[j]        \n",
    "preds = sess.run(yhat_all, feed_dict)\n",
    "top_25_idx = np.argsort(preds, axis=1)[...,-25:]\n",
    "#print(top_25_idx.shape)\n",
    "clicked_it_idx = in_idx[-1]\n",
    "submission = np.flip(top_25_idx[-1]) # decending order of impression\n",
    "print(\"query: \")\n",
    "print(\"clicked_item_id: \", clicked_it_idx)\n",
    "print(\"submission: \", submission)\n",
    "print(\"score: \", [preds[-1][i] for i in submission])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
